{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWW4sUw6QEgp"
      },
      "outputs": [],
      "source": [
        "#This file contains the implementations from scratch of \n",
        "#numerous machine learning algorithms and auxiliary functions.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "q2ythDv0Rb_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to plot decision boundary"
      ],
      "metadata": {
        "id": "Gg79ijKIRD2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_decision_boundary(model, X, yp,feature1,feature2, plot_step=0.02, cmap='viridis', alpha=0.8):\n",
        "  #plot_step is the step size for the meshgrid used to plot the decision boundary.\n",
        "  #Aplha is a value used to plot the decision regions\n",
        "  x_min = X.iloc[:, 0].min() - 0.5\n",
        "  x_max = X.iloc[:, 0].max() + 0.5\n",
        "  y_min = X.iloc[:, 1].min() - 0.5 \n",
        "  y_max = X.iloc[:, 1].max() + 0.5 \n",
        "  xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
        "                       np.arange(y_min, y_max, plot_step))\n",
        "  Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "  Z = Z.reshape(xx.shape)\n",
        "  plt.contourf(xx, yy, Z, cmap=cmap, alpha=alpha)\n",
        "  plt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=yp, cmap=cmap, edgecolors='k',alpha=0.3)\n",
        "  plt.xlabel(feature1)\n",
        "  plt.ylabel(feature2)\n",
        "  plt.title('Decision Boundary') "
      ],
      "metadata": {
        "id": "4Ac-tiRSRDkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MSE Calculation of Repeated 5 fold cross validation (model = DecisionTreeClassifier)"
      ],
      "metadata": {
        "id": "5wTBsIeZRQqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def repeated_5_fold_validation_mse(X,Y,n_repeats,n_splits):\n",
        "  mse_final=[]\n",
        "  for i in range(n_repeats):\n",
        "    mse=[]\n",
        "    for j in range(n_splits):\n",
        "      xd_train,xd_test,yd_train,yd_test=train_test_split(X,Y,test_size=(1/n_splits))\n",
        "      dt5=DecisionTreeRegressor(max_depth=10,min_samples_leaf=1)\n",
        "      dt5.fit(xd_train,yd_train)\n",
        "      yd_pred=dt5.predict(xd_test)\n",
        "      m=mean_squared_error(yd_test,yd_pred)\n",
        "      mse.append(m)\n",
        "    mse_5_fold=sum(mse)/len(mse)\n",
        "    mse_final.append(mse_5_fold)\n",
        "  return(sum(mse_final)/len(mse_final))"
      ],
      "metadata": {
        "id": "3XqyLHXAROqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DecisionTreeClassifier from Scratch "
      ],
      "metadata": {
        "id": "S0pMnJuiS_nC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Gini Index and ID3 algorithm"
      ],
      "metadata": {
        "id": "M-n6uI_YTosT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gini_index(coll):\n",
        "  l=list(coll)\n",
        "  ol=[]\n",
        "  for i in l:\n",
        "    if i not in ol:\n",
        "      ol.append(i)\n",
        "\n",
        "  freq=[]\n",
        "  for i in ol:\n",
        "    a=l.count(i)\n",
        "    freq.append(a)\n",
        "\n",
        "  total=len(l)\n",
        "  sq=[]\n",
        "  for i in freq:\n",
        "    sq.append((i/total)**2)\n",
        "\n",
        "  ans=1-sum(sq)\n",
        "  return(ans)\n",
        "\n",
        "def min_gini_indices(X):\n",
        "\n",
        "  keys=[i for i in X.columns]\n",
        "  ginivals=[]\n",
        "  for i in keys:\n",
        "    ginivals.append(gini_index(X[i]))\n",
        "  ##Converting the dictionary into a dictionary sorted by values\n",
        "  gg=dict(zip(keys,ginivals))\n",
        "  gini=sorted(gg.items(),key=lambda x:x[1])\n",
        "  giniindex=dict(gini)\n",
        "  columns=list(giniindex.keys())\n",
        "  print(columns)\n",
        "  column=columns[0]\n",
        "  return(column)\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "_Pq3cPcvRI3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This function will return threshold value\n",
        "#Where if t<threshold, then t belongs to feature\n",
        "#Else to list2\n",
        "def cont_to_cat(feature,target):\n",
        "  target=list(target)   \n",
        "  feature=list(feature)\n",
        "  feature.sort()\n",
        "  running_mean=[]\n",
        "  for i in range(len(feature)-1):\n",
        "    k=((feature[i]+feature[i+1])/2)\n",
        "    running_mean.append(k)\n",
        "  gini_impurity=[]\n",
        "  for i in range(len(running_mean)):\n",
        "    temp_threshold=running_mean[i]\n",
        "    left_leaf=[]\n",
        "    right_leaf=[]\n",
        "    for j in range(len(target)):\n",
        "      if(feature[j]<temp_threshold):\n",
        "        left_leaf.append(target[j])\n",
        "      else:\n",
        "        right_leaf.append(target[j])\n",
        "    g_left=gini_index(left_leaf)\n",
        "    g_right=gini_index(right_leaf)\n",
        "    weight_left=len(left_leaf)\n",
        "    weight_right=len(right_leaf)\n",
        "    length = weight_left + weight_right \n",
        "    G=(((g_left * weight_left) + (g_right * weight_right))/ length)\n",
        "    gini_impurity.append(G)\n",
        "\n",
        "  ind=gini_impurity.index(min(gini_impurity))\n",
        "  threshold=running_mean[ind]\n",
        "  \n",
        "  return(threshold)"
      ],
      "metadata": {
        "id": "ZEDXcJ-fTIf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Returns 2 X-dataframes which will  be the left and right respective leaves\n",
        "def binning(X1,column,threshold):\n",
        "  #X is the feature dataframe\n",
        "  #column is the column which we want to split\n",
        "  X2=pd.DataFrame()\n",
        "  X2=X1.copy(deep=True)\n",
        "  for i in range(len(X2[column])):\n",
        "    # print(X1.loc[i,column])\n",
        "    if(X2.loc[i,column]<threshold):\n",
        "      X2.loc[i,column]=0\n",
        "    else:\n",
        "      X2.loc[i,column]=1 #Right leaf is denoted by 1 by left leaf by 0\n",
        "  X2.sort_values(by=column,inplace=True)\n",
        "  X2.reset_index(inplace=True,drop=True)\n",
        "  index=0\n",
        "  fis=list(X2[column])[0]  \n",
        "  for i in range(len(X2[column])):\n",
        "    if(X2.loc[i,column]!=fis):\n",
        "      index=i\n",
        "      break\n",
        "  X2.drop(column,inplace=True,axis=1)\n",
        "  df1=X2.iloc[:index,:] \n",
        "  df2=X2.iloc[index:,:]\n",
        "  df1.reset_index(inplace=True,drop=True) \n",
        "  df2.reset_index(inplace=True,drop=True)  \n",
        "\n",
        "  return(df1,df2)"
      ],
      "metadata": {
        "id": "dV5ErHSTTLZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mode_dataframe(U):\n",
        "  list1=list(U)#U is a column\n",
        "  og_list1=[]\n",
        "  for i in list1:\n",
        "    if i not in og_list1:\n",
        "      og_list1.append(i)\n",
        "  freq=[]\n",
        "  for i in og_list1:\n",
        "    freq.append(list1.count(i))\n",
        "  ind=freq.index(max(freq))\n",
        "  max_ele=og_list1[ind]\n",
        "  return(max_ele)"
      ],
      "metadata": {
        "id": "1sRXQxmcTSJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DecisionTree(XX,yy,max_depth,dtree,depth=0):\n",
        "  #Using sorted_gini_indices function , we will get a dictionary\n",
        "  #with sorted values of gini_indices at each iteration of out function\n",
        "  #i.e DecisionTree\n",
        "  # Y=y.to_frame()\n",
        "  XX.reset_index(inplace=True,drop=True)\n",
        "  yy.reset_index(inplace=True,drop=True)\n",
        "  Xg=XX.copy(deep=True)\n",
        "  yg=yy.copy(deep=True)\n",
        "\n",
        "  if(len(Xg.columns)==3):\n",
        "    dtree['class']=mode_dataframe(yg)\n",
        "    return(dtree)\n",
        "\n",
        "  #Condition for the algorithm to self-identify when there will be no\n",
        "  #further change in the gini impurity\n",
        "  #(analogous to no further change in gini-impurity)\n",
        "  flag=0\n",
        "  for i in Xg.columns:\n",
        "    a=gini_index(Xg[i])\n",
        "    if(a!=0):\n",
        "      flag=1\n",
        "      break\n",
        "\n",
        "  if(flag==0):\n",
        "    return(dtree)  \n",
        "\n",
        "  #Condition for max_depth: If depth>max_depth then return\n",
        "  if(depth>max_depth):\n",
        "    return(dtree)\n",
        "  data=pd.concat([Xg,yg],axis=1)\n",
        "  # print(\"Prinitng data :- \"+str(depth) ,data)\n",
        "  \n",
        "  # dtree={}\n",
        "  \n",
        "  #Is a dictionary of sorted gini indices values\n",
        "  #We will use the attribute which comes first in the giniindex dictionary\n",
        "  #We will also be using the threshold_values dictionary to get the \n",
        "  #threshold of the respective column\n",
        "  column=min_gini_indices(Xg)\n",
        "  # print('For depth= '+str(depth)+' the column with min gini index is '+str(column))\n",
        "  df1,df2=binning(data,column,threshold_values[column])\n",
        "  # if(len(dtree)==0):\n",
        "  dtree[column]=dict()\n",
        "  dtree[column][0]=dict()\n",
        "  dtree[column][1]=dict()\n",
        "  # print(df1,df2)\n",
        "  X1=df1.iloc[:,:-1]\n",
        "  y1=df1.iloc[:,-1]\n",
        "  X2=df2.iloc[:,:-1]\n",
        "  y2=df2.iloc[:,-1]\n",
        "  # print(\"The depth is \"+str(depth),dtree)\n",
        "  # print(depth)\n",
        "  DecisionTree(X1,y1,max_depth,dtree[column][0],depth+1)\n",
        "  DecisionTree(X2,y2,max_depth,dtree[column][1],depth+1)"
      ],
      "metadata": {
        "id": "hjsc3g7aTTOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The test function takes a row in the series:\n",
        "#And outputs the predicted class\n",
        "def classification_dtree(tdata):\n",
        "  tdata=list(tdata)\n",
        "  cols=list(threshold_values.keys())\n",
        "  # print(cols)\n",
        "  for i in range(len(cols)):\n",
        "    # print(cols[i])\n",
        "    thresh=threshold_values[cols[i]]\n",
        "    if(tdata[i]<thresh):\n",
        "      tdata[i]=0\n",
        "    else:\n",
        "      tdata[i]=1\n",
        "  binned=dict(zip(cols,tdata))\n",
        "\n",
        "  c=str()\n",
        "  classified=-1\n",
        "  stack=[dptree]\n",
        "  val=-1\n",
        "  latest_column=list(dptree.keys())[0]\n",
        "  while(stack):\n",
        "    curr_dict=stack.pop()\n",
        "    kkeys=list(curr_dict.keys())\n",
        "    #Length of the kkeys list will always be 1\n",
        "    if 'class' in kkeys:\n",
        "      classified=curr_dict['class']\n",
        "      break\n",
        "    if(type(kkeys[0])==str):\n",
        "      latest_column=kkeys[0]\n",
        "      new_curr_dict=curr_dict[latest_column]\n",
        "      stack.append(new_curr_dict)\n",
        "    else:\n",
        "      val=binned[latest_column]\n",
        "      new_curr_dict=curr_dict[val]\n",
        "      stack.append(new_curr_dict)\n",
        "\n",
        "  return(classified)"
      ],
      "metadata": {
        "id": "oa1GVu3TTZNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gaussian Naive Bayes Classifier from scratch"
      ],
      "metadata": {
        "id": "k_eoctWjTvsN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GaussianClassifer:\n",
        "  \n",
        "  # def __init__(self,typee):\n",
        "  #   self.typee=typee  \n",
        "\n",
        "  def __cov(self,X_train):\n",
        "    self.X_train=X_train\n",
        "    X_=self.X_train\n",
        "    col=[columns for columns in X_]\n",
        "    d=len(col)#Dimensions\n",
        "    cov=np.zeros((d,d)) \n",
        "    for i in range(len(col)):\n",
        "      for j in range(len(col)):\n",
        "        A=list(X_[col[i]])\n",
        "        B=list(X_[col[j]])\n",
        "        amean=sum(A)/len(A)\n",
        "        bmean=sum(B)/len(B)\n",
        "        sum1=0\n",
        "        for k in range(len(A)):\n",
        "          adiff=A[k]-amean\n",
        "          bdiff=B[k]-bmean\n",
        "          prod=adiff*bdiff\n",
        "          sum1+=prod\n",
        "        c=sum1/(len(A)-1)\n",
        "        cov[i][j]=c  \n",
        "    return(cov)\n",
        "\n",
        "  def train(self,X_train,y_train):\n",
        "    self.X_train=X_train\n",
        "    self.y_train=y_train\n",
        "    X_train=self.X_train\n",
        "    y_train=self.y_train\n",
        "    col=[columns for columns in X_train]     \n",
        "    #Mean vector is thus made which is a column vector  \n",
        "    d=len(col)#Dimensions\n",
        "    data=pd.concat([X_train,y_train],axis=1)\n",
        "    colm=data.columns\n",
        "    classes=data[colm[-1]].unique()\n",
        "    class_data = {c: data[data[colm[-1]] == c] for c in classes}\n",
        "    #prior list consists of all prior values of classes in order\n",
        "    #class_data is a dictionary that consists of classes as keys and dataframes\n",
        "    #as values of only those classes \n",
        "    prior=[]\n",
        "    # print(len(class_data[classes[0]]))\n",
        "    for i in classes:\n",
        "      p=len(class_data[i])/len(y_train)\n",
        "      prior.append(p)  \n",
        "    n_classes=len(classes)\n",
        "    means=[]\n",
        "    #Array of mean vectors for each class:-\n",
        "    for i in class_data:\n",
        "      mm=[]\n",
        "      xx=class_data[i].iloc[:,:-1]\n",
        "      for columns in xx:\n",
        "        l=list(xx[columns])\n",
        "        u=sum(l)/len(l)\n",
        "        mm.append(u)\n",
        "      m=np.array(mm)  \n",
        "      means.append(m)\n",
        "\n",
        "    likelihood=[]\n",
        "    covv=[]\n",
        "    #Likelihood will consist of 3(# of classes) different lists\n",
        "    #Each list containing class conditional of all rows of the dataframe\n",
        "    for i in class_data:\n",
        "      X=class_data[i].iloc[:,:-1]\n",
        "      X.reset_index(drop=True, inplace=True)\n",
        "      # print(X)\n",
        "      cov1=self.__cov(X) \n",
        "      covv.append(cov1)      \n",
        "      inv_cov1=np.linalg.inv(cov1)\n",
        "      det=abs(np.linalg.det(cov1))\n",
        "      det_sqrt=det**0.5\n",
        "      count=0\n",
        "      pxwj=[]\n",
        "      xcol=[cols for cols in X]\n",
        "      for j in range(len(X)):\n",
        "        x=[]  \n",
        "        # X[col[0]][0]\n",
        "        # print(xcol)\n",
        "        for k in xcol:\n",
        "          # print(k,j)\n",
        "          b=X[k][j]\n",
        "          x.append(b)\n",
        "        #Making a vector x\n",
        "        x=np.array(x)\n",
        "\n",
        "        x_u=x-means[count]\n",
        "        # print((x_u)*(inv_cov1))\n",
        "        t=-0.5*(x_u.dot(inv_cov1).dot(np.transpose(x_u)))\n",
        "        # print(t)\n",
        "        denom=((2*math.pi)**(d/2))*(det_sqrt)\n",
        "        e=math.exp(t)\n",
        "        p=(1/denom)*e\n",
        "\n",
        "        pxwj.append(p)\n",
        "      count+=1\n",
        "      likelihood.append(pxwj)\n",
        "\n",
        "    self.prior=prior\n",
        "    self.likelihood=likelihood\n",
        "    # self.posterior=posterior\n",
        "    self.class_data=class_data\n",
        "    self.classes=classes\n",
        "    self.means=means\n",
        "    self.covv=covv\n",
        "    # print(means)\n",
        "\n",
        "  def predict(self,Xt):\n",
        "    self.Xt=Xt\n",
        "    X=self.Xt\n",
        "    prior=self.prior\n",
        "    means=self.means\n",
        "    classes=self.classes\n",
        "    likelihood=self.likelihood\n",
        "    covv=self.covv\n",
        "    x=np.array(X)\n",
        "    posterior=[]\n",
        "    d=len(Xt)\n",
        "    for i in range(len(means)):\n",
        "      x_u=x-means[i]\n",
        "      inv_cov1=np.linalg.inv(covv[i])\n",
        "      t=-0.5*(x_u.dot(inv_cov1).dot(np.transpose(x_u)))\n",
        "      det=abs(np.linalg.det(covv[i]))\n",
        "      det_sqrt=det**0.5\n",
        "      denom=((2*math.pi)**(d/2))*(det_sqrt)\n",
        "      e=math.exp(t)\n",
        "      p=(1/denom)*e\n",
        "      p=p*prior[i]\n",
        "      posterior.append(p)\n",
        "    #length of posterior list will be equal to no. of classes\n",
        "    ind=posterior.index(max(posterior))\n",
        "    # print(classes[ind])\n",
        "    return(classes[ind])\n",
        "\n",
        "  def test(self,X_test,y_test):\n",
        "    # Will output [[predicted_classes], accuracy]\n",
        "    data=pd.concat([X_test,y_test],axis=1)\n",
        "    data.reset_index(drop=True, inplace=True)\n",
        "    X=data.iloc[:,:-1]\n",
        "    y=list(data.iloc[:,-1])\n",
        "    predicted=[]\n",
        "    cols=[i for i in X]\n",
        "    length=len(X[cols[0]])\n",
        "    # print(length)\n",
        "    for i in range(length):  \n",
        "      x=[]\n",
        "      for k in cols:\n",
        "        x.append(X[k][i])\n",
        "      predicted.append(self.predict(x))\n",
        "    correct=0\n",
        "    for i in range(len(predicted)):\n",
        "      if(predicted[i]==y[i]):\n",
        "        correct+=1\n",
        "    accuracy=(correct/len(y_test))*100\n",
        "    op=[accuracy,predicted]\n",
        "    return(op)"
      ],
      "metadata": {
        "id": "hznW7cTWTuPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bagging from scratch"
      ],
      "metadata": {
        "id": "LoLzrTpeUdAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Baggy:\n",
        "\n",
        "  def __init__(self,base_estimator,n_estimators=10):\n",
        "    self.base_estimator=base_estimator\n",
        "    self.n_estimators=n_estimators\n",
        "    #base estimator is the ML model whose ensemble is going\n",
        "    #to be considered n_estimators number of times to get the final prediction\n",
        "\n",
        "  def fit(self,Xt,yt):\n",
        "    self.bag=[]\n",
        "    #This bag will contain all the models trained on the base \n",
        "    #eastimators, for n_estimators number of times\n",
        "    \n",
        "    for i in range(self.n_estimators):\n",
        "      dft=pd.concat([Xt,yt],axis=1)\n",
        "      sample_df=dft.sample(n=len(yt),replace=True)\n",
        "      Xtrain=sample_df.iloc[:,:-1]\n",
        "      ytrain=sample_df.iloc[:,-1]\n",
        "      model=self.base_estimator\n",
        "      model.fit(Xtrain,ytrain)\n",
        "      self.bag.append(model)\n",
        "\n",
        "  def predict(self,Xtst):\n",
        "    initial_pred_list=[]\n",
        "    #Is a list of length n_estimators(list of lists)\n",
        "    #It contains the y_pred of all the n_estimators number of models\n",
        "    for i in range(len(self.bag)):\n",
        "      ypred=(self.bag[i]).predict(Xtst)\n",
        "      initial_pred_list.append(ypred)\n",
        "    y_pred_final=[]\n",
        "    for i in range(len(initial_pred_list[0])):\n",
        "      temp_list=[]\n",
        "      for j in range(len(initial_pred_list)):\n",
        "        temp_list.append(initial_pred_list[j][i])\n",
        "      y_pred_final.append(mode(temp_list))\n",
        "    #We consider each value of y_pred based on majority voting of each index from \n",
        "    #all the n_estimator no. of lists\n",
        "    return(y_pred_final)    \n"
      ],
      "metadata": {
        "id": "b5glwGTuUctI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KMeans Clustering from scratch"
      ],
      "metadata": {
        "id": "jXLQ4sjuVLEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Kmeansss:\n",
        "  def __init__(self,n_clusters=40,max_iter=100):\n",
        "    self.n_clusters=n_clusters#Cluster value k\n",
        "    self.max_iter=max_iter# Max iterations\n",
        "\n",
        "\n",
        "  def fit(self,X):\n",
        "    Xp=np.array(X)\n",
        "    n_samples,n_features=Xp.shape\n",
        "    centroids = np.zeros((self.n_clusters,n_features))\n",
        "    for i in range(self.n_clusters): # iterations of \n",
        "      centroid = Xp[np.random.choice(range(n_samples))] # random centroids\n",
        "      centroids[i] = centroid\n",
        "\n",
        "\n",
        "    for z in range(self.max_iter):\n",
        "\n",
        "      dist=[]#Contains the euclidean distance of each point from each centroid\n",
        "      #len(dist)=n_clusters\n",
        "      #len(dist[i])=400(i.e n_samples)\n",
        "      for i in range(len(centroids)):\n",
        "        clus=[]\n",
        "        x=centroids[i]\n",
        "        for j in range(n_samples):\n",
        "          y=Xp[j]\n",
        "          d=np.linalg.norm(x-y)\n",
        "          clus.append(d)\n",
        "        clus=np.array(clus)\n",
        "        dist.append(clus)\n",
        "\n",
        "      dist=np.array(dist)\n",
        "      \n",
        "\n",
        "      labels=np.zeros(n_samples)\n",
        "\n",
        "      for i in range(len(dist[0])):\n",
        "        lk=[]\n",
        "        for j in range(len(dist)):\n",
        "          lk.append(dist[j][i])\n",
        "        ind=lk.index(min(lk))\n",
        "        labels[i]=ind  \n",
        "\n",
        "      c=np.array([i for i in centroids])\n",
        "      for j in range(self.n_clusters):\n",
        "        centroids[j] = np.mean(X[labels == j], axis=0)\n",
        "\n",
        "      flag=0\n",
        "      if(np.array_equal(c,centroids)):#Condition for convergence\n",
        "          flag=1\n",
        "\n",
        "      if(flag==1):\n",
        "        break\n",
        "      # print(len(labels))\n",
        "\n",
        "    self.labels_=labels#Cluster labels\n",
        "    self.cluster_centers_=centroids#Cluster centers stored\n",
        "\n"
      ],
      "metadata": {
        "id": "fOVnJu35VKNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PCA(Principal Component Analysis) from scratch"
      ],
      "metadata": {
        "id": "ha1VY9rlVevC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#PCA implementation:-\n",
        "\n",
        "#Reduced no. of features is given by reduced_features argument\n",
        "\n",
        "class PCA:\n",
        "  #data has to be a pandas DataFrame\n",
        "  def __init__(self,data,reduced_features):\n",
        "    self.data=data\n",
        "    self.reduced_features=reduced_features\n",
        "\n",
        "  def __covv(self,X,Y):\n",
        "\n",
        "    xmean=sum(X)/len(X)\n",
        "    ymean=sum(Y)/len(Y)\n",
        "    sum1=0\n",
        "    for i in range(len(X)):\n",
        "      xdiff=X[i]-xmean\n",
        "      ydiff=Y[i]-ymean\n",
        "      prod=xdiff*ydiff\n",
        "      sum1+=prod\n",
        "    cov=sum1/(len(X)-1)\n",
        "    return(cov)\n",
        "\n",
        "\n",
        "  def __cov_mat(self,data1):\n",
        "    \n",
        "    cov_matrix=[]\n",
        "\n",
        "    for i in range(len(data1.columns)):\n",
        "      a=[]\n",
        "      for j in range(len(data1.columns)):\n",
        "        x=list(data1.iloc[:,i])\n",
        "        y=list(data1.iloc[:,j])\n",
        "        c=self.__covv(x,y)\n",
        "        a.append(c)\n",
        "      a=np.array(a)\n",
        "      cov_matrix.append(a)\n",
        "\n",
        "    cov_matrx=np.array(cov_matrix)\n",
        "\n",
        "    return(cov_matrx)\n",
        "\n",
        "#Code for covariance matrix from scratch is done\n",
        "\n",
        "  def transformed_data(self):\n",
        "    dataset=self.data\n",
        "    cov_mat=self.__cov_mat(dataset)\n",
        "\n",
        "    #Computing the eigenvalues and eigenvectors of Covariance matrix\n",
        "    eig_vals , eig_vecs = np.linalg.eig(cov_mat)\n",
        "\n",
        "    sorted_index=np.argsort(eig_vals)[::-1]\n",
        "    sorted_eig_vals = eig_vals[sorted_index]\n",
        "    sorted_eig_vecs = eig_vecs[:,sorted_index]\n",
        "\n",
        "    selected_eig_vec = sorted_eig_vecs[:,:self.reduced_features]\n",
        "\n",
        "    np_data= np.array(dataset)\n",
        "\n",
        "    # np_data = (np_data - np.mean(np_data , axis=0))/(np.std(np_data,axis=0))\n",
        "    transformed= np.dot(np_data,selected_eig_vec)\n",
        "    transformed_df = pd.DataFrame(transformed)\n",
        "\n",
        "    self.eigen_values_=eig_vals\n",
        "    self.eigen_vectors_=eig_vecs\n",
        "    return(transformed_df)\n",
        "\n",
        "  def get_cov(self,data1):\n",
        "    return(self.__cov_mat(data1))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rtssvTB_VeEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LDA (Linear Discriminant Analysis) from scratch"
      ],
      "metadata": {
        "id": "PEA39xriVujH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LDA:\n",
        "  def __init__(self,dataset=None,variance=None):\n",
        "    self.data=dataset\n",
        "    self.variance=variance\n",
        "\n",
        "  def __matrices(self,Xl,yl):\n",
        "    X=np.array(Xl)\n",
        "    y=np.array(yl)\n",
        "    rows,cols=X.shape\n",
        "    uniq_classes=np.unique(y)\n",
        "    temp_scat=np.cov(X.T)*(rows-1)\n",
        "\n",
        "    scatter_intra=0\n",
        "    for i in range(len(uniq_classes)):\n",
        "      class_items=np.flatnonzero(y==uniq_classes[i])\n",
        "      scatter_intra = scatter_intra + np.cov(X[class_items].T)*(len(class_items)-1)\n",
        "\n",
        "\n",
        "    scatter_inter = temp_scat - scatter_intra\n",
        "    \n",
        "    return(scatter_intra,scatter_inter)\n",
        "\n",
        "  # def __auto_select(self,eig_vals,eig_vecs):\n",
        "    \n",
        "  def __abss(self,x):\n",
        "    return(abs(x))\n",
        "\n",
        "  def transform(self,Xp,yp,linear_discriminants):\n",
        "    scatter_intra,scatter_inter=self.__matrices(Xp,yp)\n",
        "    self.Sw_=scatter_intra\n",
        "    self.Sb_=scatter_inter\n",
        "    inv_scat_intra=np.linalg.pinv(scatter_intra)\n",
        "    eig_vals,eig_vectors = np.linalg.eig(np.dot(inv_scat_intra,scatter_inter)) \n",
        "    X=np.array(Xp)\n",
        "    sorted_index=np.argsort(eig_vals)[::-1]\n",
        "    sorted_eig_vals = eig_vals[sorted_index]\n",
        "    sorted_eig_vecs = eig_vectors[:,sorted_index]\n",
        "    self.sorted_eig_vecs_=sorted_eig_vecs\n",
        "    xpx=np.dot(X,sorted_eig_vecs)\n",
        "    self.default_var_conserved_=xpx.var()\n",
        "    if(self.variance==None):\n",
        "      total_var= xpx.var()#Amount of variance to be preserved\n",
        "    else:\n",
        "        total_var=self.variance\n",
        "\n",
        "    n_components = -1\n",
        "    for i in range(1,len(Xp)+1):\n",
        "      XX=np.dot(X,sorted_eig_vecs[:,:i])\n",
        "      temp_var=XX.var()\n",
        "      \n",
        "      if(temp_var >= total_var):\n",
        "        n_components=i\n",
        "        break\n",
        "\n",
        "    #Auto-selection done\n",
        "    if(linear_discriminants!=-1):\n",
        "      n_components=linear_discriminants\n",
        "    selected_eig_vecs = sorted_eig_vecs[:,:n_components]\n",
        "    transformed_df = pd.DataFrame(np.dot(X,selected_eig_vecs))\n",
        "    tdf = transformed_df.applymap(self.__abss)\n",
        "    self.df=tdf\n",
        "    self.y=yp\n",
        "    return(tdf)\n",
        "\n",
        "  def _roc_multiclass(self,model,Xtest,ytest):\n",
        "    #n_classes is no. of unique classes\n",
        "    n_classes= len(list(self.y.unique()))\n",
        "    y_prob=model.predict_proba(Xtest)\n",
        "    fpr={}\n",
        "    tpr={}\n",
        "    roc_auc={}\n",
        "    for i in range(n_classes):\n",
        "      fpr[i], tpr[i], _ = roc_curve(ytest == i, y_prob[:, i])#Calculates the \n",
        "      #Probab. of each element in class i is given by y_prob[:,i]\n",
        "      #ytest==i returns Boolean mask of true-false values, giving true \n",
        "      #for class==i only\n",
        "      roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(np.eye(n_classes)[ytest.ravel()], y_prob.ravel())\n",
        "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "\n",
        "    return(fpr,tpr,roc_auc)      \n",
        "\n",
        "  def lda_classifier_roc_auc_5_fold(self,K_fold=5):\n",
        "    \n",
        "    #We will plot by micro averaging\n",
        "\n",
        "\n",
        "    for i in range(K_fold):\n",
        "      X_tr,X_te,y_tr,y_te=train_test_split(self.df,self.y)\n",
        "      model=GaussianNB()\n",
        "      model.fit(X_tr,y_tr)\n",
        "      fpr1,tpr1,roc_auc1=self._roc_multiclass(model,X_te,y_te)\n",
        "      colors = ['red', 'green', 'blue']\n",
        "      # plt.figure()\n",
        "      print(fpr1)\n",
        "      print(roc_auc1)\n"
      ],
      "metadata": {
        "id": "Vc10nuLlVx8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bi-Directional Feature Selection"
      ],
      "metadata": {
        "id": "e4epnEJJV-7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def bi_directional_feature_selection(X, y, n_features):\n",
        "\n",
        "    # create decision tree classifier\n",
        "    dt = DecisionTreeClassifier()\n",
        "\n",
        "    # create RFECV object with StratifiedKFold cross-validation\n",
        "    rfecv = RFECV(estimator=dt, step=1, cv=StratifiedKFold(5),\n",
        "                  scoring='accuracy', n_jobs=-1, verbose=0)\n",
        "\n",
        "    # perform forward feature selection\n",
        "    rfecv.fit(X, y)\n",
        "\n",
        "    # select the top n_features from forward selection\n",
        "    fwd_features = np.array(X.columns)[rfecv.support_]\n",
        "    fwd_X = X[fwd_features]\n",
        "\n",
        "    # perform backward feature selection\n",
        "    bkwd_features = fwd_features.tolist()\n",
        "    bkwd_X = fwd_X.copy()\n",
        "    while len(bkwd_features) > n_features:\n",
        "        # fit decision tree classifier\n",
        "        dt.fit(bkwd_X, y)\n",
        "\n",
        "        # calculate feature importances\n",
        "        feature_importances = dt.feature_importances_\n",
        "\n",
        "        # find the least important feature\n",
        "        least_important_feature_idx = np.argmin(feature_importances)\n",
        "        least_important_feature = bkwd_features[least_important_feature_idx]\n",
        "\n",
        "        # remove the least important feature from the feature set\n",
        "        bkwd_features.remove(least_important_feature)\n",
        "        bkwd_X = fwd_X[bkwd_features]\n",
        "\n",
        "    # return the final feature set\n",
        "    return bkwd_X\n"
      ],
      "metadata": {
        "id": "9i8BwCJfV-uL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network from scratch"
      ],
      "metadata": {
        "id": "fINPQ6E0XCXp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Artificial Neural Network (ANN) using MLP (Multi Layer Perceptron)**"
      ],
      "metadata": {
        "id": "n7QAr1ChXRe7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP_ANN:\n",
        "  def __init__(self,layers,learning_rate=0.01,activation_function='sigmoid',weights=None):\n",
        "    #By default the activation fucntion is kept defined as sigmoid\n",
        "    #layers is a list that contains the no. of neurons\n",
        "    #included in each layer\n",
        "    self.layers = layers\n",
        "    self.activation_function = activation_function\n",
        "    self.learning_rate = learning_rate\n",
        "    if(weights==None):\n",
        "      self.weights = [np.random.rand(layers[i],layers[i+1]) for i in range(len(layers)-1)]\n",
        "    else:\n",
        "      self.weights = weights  \n",
        "  #Specify the name of the activation function\n",
        "\n",
        "  def activate(self,x,name='sigmoid'):\n",
        "    #Takes x as a vector input\n",
        "    #As each function is defined using numpy,\n",
        "    #Thus , even if x is a vector, the np function will output a vector\n",
        "    #After all elements have been acted upon by the np function\n",
        "    x= x.astype(float)\n",
        "    if(name=='sigmoid'):\n",
        "      # print(x)\n",
        "      h = 1/(1+np.exp(-x))\n",
        "      # print(h)\n",
        "      return(h)\n",
        "    elif(name=='ReLU'):\n",
        "      return(np.maximum(x,0))  \n",
        "    elif(name=='tanh'):\n",
        "      return(np.tanh(x))\n",
        "    elif(name=='Leaky ReLU'):\n",
        "      return(np.maximum(0.1*x,x))\n",
        "    else:\n",
        "      raise ValueError(\"Invalid Activation Function\")      \n",
        "\n",
        "  def forward_propagation(self,inputs):\n",
        "    #Gives outputs\n",
        "    activations = [inputs]\n",
        "    for w in self.weights:\n",
        "      #w is a weight vector which contains the 'feature no.' of weights\n",
        "      #for each neuron\n",
        "      # net_input = np.dot(np.reshape(activations[-1], (1,w.shape[0])), w)\n",
        "      # net_input = np.dot(activations[-1].reshape(-1,1).T, w) correct one\n",
        "      net_input = np.dot(activations[-1].reshape(-1,w.shape[0]), w)\n",
        "      # net_input = np.dot(w.T,activations[-1])\n",
        "      #This is the input calculated after giving the edges their corresponding\n",
        "      #Weights from the previous layer  \n",
        "      layer_activations=self.activate(net_input,name=self.activation_function)\n",
        "      #Acted upon by the activation function\n",
        "      activations.append(layer_activations)\n",
        "\n",
        "    self.activations = activations\n",
        "    #The activations list contains all the inputs going into every layer\n",
        "    #But we return only the outputs of the output layer\n",
        "    return(activations[-1])  \n",
        "\n",
        "  def __activation_derivative(self,x,name='sigmoid'):\n",
        "    if(name==\"sigmoid\"):\n",
        "      return( (self.activate(x)*(1-self.activate(x))) )\n",
        "    elif(name=='ReLU'):\n",
        "      return( (x>0.0).astype(float) )\n",
        "    elif(name=='tanh'):\n",
        "      return(1-x**2)\n",
        "    elif(name=='Leaky ReLU'):\n",
        "      if(x<0.0):\n",
        "        return(0.1)\n",
        "      else:\n",
        "        return(x)\n",
        "    else:\n",
        "      raise ValueError(\"Invalid Activation Function\")\n",
        "\n",
        "  def backward_propagation(self, inputs, outputs, act_vals):\n",
        "    # Perform backward propagation and update the weights and biases\n",
        "    error = act_vals - outputs\n",
        "    deltas = [error * self.__activation_derivative(outputs)]\n",
        "    for i in reversed(range(len(self.layers)-1)):\n",
        "      layer_output = self.activations[i+1]\n",
        "      # print(deltas[-1].shape,self.weights[i].shape)\n",
        "      kp = np.dot(deltas[-1], self.weights[i].T)\n",
        "      ap = self.__activation_derivative(layer_output)\n",
        "      print(kp.shape,ap.shape )\n",
        "      delta = np.dot(kp.T , ap )\n",
        "      print(delta.shape)\n",
        "      deltas.append(delta)\n",
        "    deltas = list(reversed(deltas))\n",
        "\n",
        "    for i in range(len(self.layers)-1):\n",
        "      print(self.activations[i].shape,deltas[i].shape)\n",
        "      weight_delta = np.dot(self.activations[i], deltas[i])\n",
        "      bias_delta = np.sum(deltas[i], axis=0)\n",
        "      self.weights[i] += weight_delta * self.learning_rate\n",
        "      self.biases[i] += bias_delta * self.learning_rate\n",
        "\n",
        "    return error\n",
        "\n",
        "\n",
        "  def train(self,inputs,act_vals,epochs):\n",
        "    #Returns a list of errors for each epoch\n",
        "    errors=[]\n",
        "    for i in range(epochs):\n",
        "      #Now we will implement Stochastoc gradient descent,\n",
        "      #i.e gradient descent for each row of inputs\n",
        "      for j in range(len(inputs)):\n",
        "        # ip = inputs[j].reshape(-1,1) correct one\n",
        "        ip = inputs[j]\n",
        "        op = self.forward_propagation(ip)\n",
        "        act_val = act_vals[j]\n",
        "        curr_error = self.backward_propagation(ip,op,act_val)\n",
        "        #backward_propagation used => self.weights is updated\n",
        "        errors.append(curr_error)\n",
        "\n",
        "      if(i%100==0):\n",
        "        mean_error = np.mean(np.abs(errors))\n",
        "        print(f\"Epoch {i}: Error = {mean_error}\")\n",
        "\n",
        "    self.errors = errors\n",
        "    return(errors)\n",
        "\n",
        "  def predict(self,inputs):\n",
        "    return(self.activations)\n",
        "\n",
        "  def accuracy(self,act_vals,outputs):\n",
        "    a = 0\n",
        "    for i in range(len(act_vals)):\n",
        "      if(list(act_vals)[i]==list(outputs)[i]):\n",
        "        a+=1\n",
        "    acc = a/len(act_vals)\n",
        "\n",
        "    return(acc)    \n"
      ],
      "metadata": {
        "id": "uIznVOacXCtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVM from Scratch"
      ],
      "metadata": {
        "id": "nV44UP8jY7nS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\n",
        "def remove_correlated_features(X):\n",
        "    corr_threshold = 0.9\n",
        "    corr = X.corr()\n",
        "    drop_columns = np.full(corr.shape[0], False, dtype=bool)\n",
        "    for i in range(corr.shape[0]):\n",
        "        for j in range(i + 1, corr.shape[0]):\n",
        "            if corr.iloc[i, j] >= corr_threshold:\n",
        "                drop_columns[j] = True\n",
        "    columns_dropped = X.columns[drop_columns]\n",
        "    X.drop(columns_dropped, axis=1, inplace=True)\n",
        "    return columns_dropped\n",
        "\n",
        "\n",
        "def remove_less_significant_features(X, Y):\n",
        "    sl = 0.05\n",
        "    regression_ols = None\n",
        "    columns_dropped = np.array([])\n",
        "    for itr in range(0, len(X.columns)):\n",
        "        regression_ols = sm.OLS(Y, X).fit()\n",
        "        max_col = regression_ols.pvalues.idxmax()\n",
        "        max_val = regression_ols.pvalues.max()\n",
        "        if max_val > sl:\n",
        "            X.drop(max_col, axis='columns', inplace=True)\n",
        "            columns_dropped = np.append(columns_dropped, [max_col])\n",
        "        else:\n",
        "            break\n",
        "    regression_ols.summary()\n",
        "    return columns_dropped\n",
        "\n",
        "\n",
        "def compute_cost(W, X, Y):\n",
        "    # calculate hinge loss\n",
        "    N = X.shape[0]\n",
        "    distances = 1 - Y * (np.dot(X, W))\n",
        "    distances[distances < 0] = 0  # equivalent to max(0, distance)\n",
        "    hinge_loss = regularization_strength * (np.sum(distances) / N)\n",
        "\n",
        "    # calculate cost\n",
        "    cost = 1 / 2 * np.dot(W, W) + hinge_loss\n",
        "    return cost\n",
        "\n",
        "\n",
        "def calculate_cost_gradient(W, X_batch, Y_batch):\n",
        "    # if only one example is passed (eg. in case of SGD)\n",
        "    if type(Y_batch) == np.float64:\n",
        "        Y_batch = np.array([Y_batch])\n",
        "        X_batch = np.array([X_batch])  # gives multidimensional array\n",
        "\n",
        "    distance = 1 - (Y_batch * np.dot(X_batch, W))\n",
        "    dw = np.zeros(len(W))\n",
        "\n",
        "    for ind, d in enumerate(distance):\n",
        "        if max(0, d) == 0:\n",
        "            di = W\n",
        "        else:\n",
        "            di = W - (regularization_strength * Y_batch[ind] * X_batch[ind])\n",
        "        dw += di\n",
        "\n",
        "    dw = dw/len(Y_batch)  # average\n",
        "    return dw\n",
        "\n",
        "\n",
        "def sgd(features, outputs):\n",
        "    max_epochs = 5000\n",
        "    weights = np.zeros(features.shape[1])\n",
        "    nth = 0\n",
        "    prev_cost = float(\"inf\")\n",
        "    cost_threshold = 0.01  # in percent\n",
        "    # stochastic gradient descent\n",
        "    for epoch in range(1, max_epochs):\n",
        "        # shuffle to prevent repeating update cycles\n",
        "        X, Y = shuffle(features, outputs)\n",
        "        for ind, x in enumerate(X):\n",
        "            ascent = calculate_cost_gradient(weights, x, Y[ind])\n",
        "            weights = weights - (learning_rate * ascent)\n",
        "\n",
        "        # convergence check on 2^nth epoch\n",
        "        if epoch == 2 ** nth or epoch == max_epochs - 1:\n",
        "            cost = compute_cost(weights, features, outputs)\n",
        "            print(\"Epoch is: {} and Cost is: {}\".format(epoch, cost))\n",
        "            # stoppage criterion\n",
        "            if abs(prev_cost - cost) < cost_threshold * prev_cost:\n",
        "                return weights\n",
        "           \n",
        "\n",
        "X_normalized = MinMaxScaler().fit_transform(X)\n",
        "X = pd.DataFrame(X_normalized)\n",
        "\n",
        "# split data into train and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# train the model\n",
        "clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# testing the model\n",
        "y_train_predicted = clf.predict(X_train)\n",
        "y_test_predicted = clf.predict(X_test)\n",
        "\n",
        "print(\"accuracy on test dataset: {}\".format(accuracy_score(y_test, y_test_predicted)))\n",
        "print(\"recall on test dataset: {}\".format(recall_score(y_test, y_test_predicted)))\n",
        "print(\"precision on test dataset: {}\".format(precision_score(y_test, y_test_predicted)))"
      ],
      "metadata": {
        "id": "M9esm4dMY7IW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}